Redis 服务上线当天，就密切关注 Redis 的一些重要监控指标（clients：客户端连接数、memory、stats：服务器每秒钟执行的命令数量、commandstats：一些关键命令的执行统计信息、redis.error.log：异常日志）。（参考自《Redis监控方案》）

观察到下午5点左右，发现“客户端连接数”一直在增长，最高时都超过了2000个（见下图），即使减少也就减1~2个。但应用的QPS却在 10 个左右，而线上应用服务器不超过10台。按理说，服务器肯定不会有这么高的连接数，肯定哪里使用有问题。

![](https://i.imgur.com/GKoWOMo.png)

现在只能通过逆向思维反向来推测问题：

- Redis服务端监控到的“客户端连接数”表明所有客户端总和起来应该有那么多，所以首先到各个应用服务器上确认连接数量；
- 通过“sudo netstat -antp | grep 6379 | wc -l”确认，有一台应用Redis的连接数都超过了1000个，另一台应用则在400左右，其它的都在60上下。（60上下是正常的）
- 第一个问题：为什么不同的机器部署了同一个应用程序，表现出来的行为却是不一样？
- 第二个问题：连接数超过1000个的那台，其请求量(140)是比其它机器(200+)要低的(因为它在Nginx中配置的权重低)，那它的连接数为什么会这么高？到底发生了什么？
- 对于“第二个问题”，我们通过各个应用的Redis异常日志（redis.error.log）知道发生了什么。最高那台应用的异常操作特别多，共有130+个异常，且存在“关闭集群链接时异常导致连接泄漏”问题；另一台较高的应用也存在类似的情况，而其它正常的应用则不超过2个异常，且不存在“连接泄漏”问题。这样，“第二个问题”算是弄清楚了。（“连接泄漏”问题具体如何修复见《[FAQ] Jedis使用过程中踩过的那些坑》）
- 至此，感觉问题好像已经解决了，但其实没有。通过连续几天的观察，发现最高的时候，它的连接数甚至超过了3000+，这太恐怖了。（当时 leader 还和我说，要不要重启一下应用）
即使应用的QPS是 20个/s，且存在“连接泄漏”问题，连接数也不会超过1000+。但现在连接数尽然达到了3000+，这说不通，只有一个可能就是未正确使用Jedis。
- 这时候就继续反推，Redis的连接数反映了Jedis对象池的池对象数量。线上部署了2台Redis服务器作为一个集群，说明这台应用共持有(3000/2=1500)个池对象。(因为Jedis基于Apache Commons Pool的GenericObjectPool实现)
- 第三个问题：根据应用的QPS，每秒钟请求需要的Active池对象也不会超过20个，那其余的1480个都是“空闲池对象”。为什么那么多的“空闲池对象”未被释放？
- 现在就来反思：Jedis的那些配置属性与对象池管理“空闲池对象”相关，GenericObjectPool背后是怎么管理“空闲池对象”的？


由于在使用Jedis的过程中，就对Apache Commons Pool摸了一次底。对最后的两个疑惑都比较了解，Jedis的以下这些配置与对象池管理“空闲池对象”相关：

    redis.max.idle.num=32768
	redis.min.idle.num=30
	redis.pool.behaviour=FIFO
	redis.time.between.eviction.runs.seconds=1
	redis.num.tests.per.eviction.run=10
	redis.min.evictable.idle.time.minutes=5
	redis.max.evictable.idle.time.minutes=1440

在上面说“每台应用的Jedis连接数在60个左右是正常的”的理由是：线上共部署了2台Redis服务器，Jedis的“最小空闲池对象个数”配置为30 (redis.min.idle.num=30)。

